{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvfolino68/NLP_CFPB_complaints_notebook/blob/master/CFPB%20Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtMKErCJ3b_K",
        "colab_type": "text"
      },
      "source": [
        "Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikwlazzBJ7l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckB5ZBV6J7l6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode,  plot\n",
        "from plotly.graph_objs import *\n",
        "init_notebook_mode()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(__doc__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JWPj_RG6yfg",
        "colab_type": "text"
      },
      "source": [
        "Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y3ek1aTJ7l-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://data.consumerfinance.gov/api/views/s6ew-h6mp/rows.csv?accessType=DOWNLOAD', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6Ti71eM0s-9",
        "colab_type": "text"
      },
      "source": [
        "Create cleaing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv_2O1gs0vQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clreate word lemmetizer object\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "def text_process(mess):\n",
        "    \"\"\"\n",
        "    Takes in a string of text, then performs the following:\n",
        "    1. Remove all punctuation\n",
        "    2. remove x, M, T\n",
        "    3. Remove all stopwords\n",
        "    4. remove any non alpha words\n",
        "    5. Returns a list of the cleaned text\n",
        "    \"\"\"\n",
        "    \n",
        "    # Check characters to see if they are in punctuation\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\n",
        "    nopunc = [char for char in mess if char not in ['X', 'M','T']]\n",
        "\n",
        "    # Join the characters again to form the string.\n",
        "    nopunc = ''.join(nopunc)\n",
        "    \n",
        "    clean_words = [lemmatizer.lemmatize(word) for word in nopunc.split() if word.lower() not in ENGLISH_STOP_WORDS ]\n",
        "    clean_words = [word for word in clean_words if word.lower().isalpha()]\n",
        "    \n",
        "    # Now just remove any stopwords\n",
        "    return [word for word in clean_words if word.lower() not in ENGLISH_STOP_WORDS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-KKqo6064FI",
        "colab_type": "text"
      },
      "source": [
        "Remove nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pGvAgCs_3RMi",
        "colab": {}
      },
      "source": [
        "bank_name = 'M&T BANK CORPORATION'\n",
        "df = df[pd.notnull(df['Consumer complaint narrative'].str.strip())]\n",
        "df2 =df[df['Company'].str.contains(bank_name)]\n",
        "df2['Consumer complaint narrative'] = df2['Consumer complaint narrative'].apply(text_process)\n",
        "df2.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQxFw0Ub3Tm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complaint_train = df2\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.5,\n",
        "                             max_features=1500,\n",
        "                             min_df=2,\n",
        "                             stop_words=ENGLISH_STOP_WORDS,\n",
        "                             analyzer=text_process,\n",
        "                             use_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(df2['Consumer complaint narrative'].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipFlw84H8fAQ",
        "colab_type": "text"
      },
      "source": [
        "Perform Matrix Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mici11bp3ayd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Performing dimensionality reduction using LSA\")\n",
        "\n",
        "# Vectorizer results are normalized, which makes KMeans behave as\n",
        "# spherical k-means for better results. Since LSA/SVD results are\n",
        "# not normalized\n",
        "\n",
        "#reduce to 3 dimensions\n",
        "dimensions = 3\n",
        "\n",
        "svd = TruncatedSVD(dimensions)\n",
        "\n",
        "normalizer = Normalizer(copy=False)\n",
        "\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "x_transformed = lsa.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOV9h-af8nT9",
        "colab_type": "text"
      },
      "source": [
        "Plot in 3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOWi5GP-8d6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = Axes3D(fig)\n",
        "ax.scatter(x_transformed[:,0], x_transformed[:,1],x_transformed[:,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoZZWILJ8wlx",
        "colab_type": "text"
      },
      "source": [
        "Do the Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUhM7cIJ8vuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-1, 1]\n",
        "    ax1.set_xlim([-.01, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(x_transformed) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=0, init='k-means++', max_iter=100, n_init=1)\n",
        "    cluster_labels = clusterer.fit_predict(x_transformed)\n",
        "    complaint_train.loc[:,'Cluster_of_{}'.format(n_clusters)] = cluster_labels+1\n",
        "    complaint_train.loc[:,'x1'] = x_transformed[ : , 0]\n",
        "    complaint_train.loc[:,'x2'] = x_transformed[ : , 1]\n",
        "    complaint_train.loc[:,'x3'] = x_transformed[ : , 2]\n",
        "\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(x_transformed, cluster_labels)\n",
        "    \n",
        "    print()\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)    \n",
        "    print(\"Top terms per cluster:\")\n",
        "\n",
        "#     if opts.n_components:\n",
        "    original_space_centroids = svd.inverse_transform(clusterer.cluster_centers_)\n",
        "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
        "\n",
        "\n",
        "#    order_centroids = clusterer.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "    terms = vectorizer.get_feature_names()\n",
        "    for i in range(n_clusters):\n",
        "        print(\"Cluster \" +str(i+1)+\":\", end='')\n",
        "        for ind in order_centroids[i, :10]:\n",
        "            print(' %s' % terms[ind], end='')\n",
        "        print()\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(x_transformed, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(x_transformed[:, 0], x_transformed[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"Customer Complaint Text Data in 2D Space\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle((\"KMeans clustering on M&T Bank Customer Complaints \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vk2VBkxC1Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly \n",
        "plotly.tools.set_credentials_file(username='mvfolino68', api_key='')\n",
        "import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "traces = []\n",
        "for i in complaint_train.Cluster_of_3.sort_values(ascending=True).unique():\n",
        "    df_by_cluster = complaint_train[complaint_train['Cluster_of_3'] == i]\n",
        "    traces.append(go.Scatter3d(\n",
        "        x=df_by_cluster['x1'],\n",
        "        y=df_by_cluster['x2'],\n",
        "        z=df_by_cluster['x3'],\n",
        "        text=df_by_cluster['Consumer complaint narrative'],\n",
        "        mode='markers',\n",
        "        opacity=0.9,\n",
        "        marker={\n",
        "            'size': 10,\n",
        "            'line': {'width': 0.5, 'color': 'white'}\n",
        "        },\n",
        "        name='Cluster #'+str(i)\n",
        "    ))\n",
        "data = traces\n",
        "layout = go.Layout(\n",
        "                scene={  \n",
        "                        'xaxis':{'type': 'linear', 'title': 'X axis'},\n",
        "                           'yaxis':{'title': 'Y axis'},\n",
        "                           'zaxis':{'type': 'linear', 'title': 'Z axis'}\n",
        "                           },\n",
        "            margin={'l': 40, 'b': 40, 't': 10, 'r': 10},\n",
        "            legend={'x': 1, 'y': 1},\n",
        "            hovermode='closest',\n",
        "            title=go.layout.Title(\n",
        "            text='M&T Bank Consumer Complaints Natural Language Processing and Machine Learning Case',\n",
        "            xref='paper',\n",
        "            x=.5, y=1)\n",
        "                    )\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "plotly.offline.plot(fig, filename='test')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}